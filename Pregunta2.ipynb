{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importamos la siguientes librerias\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cargamos nuestra base de datos \n",
    "data_set = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                            0\n",
       "airline_sentiment                   0\n",
       "airline_sentiment_confidence        0\n",
       "negativereason                   5462\n",
       "negativereason_confidence        4118\n",
       "airline                             0\n",
       "airline_sentiment_gold          14600\n",
       "name                                0\n",
       "negativereason_gold             14608\n",
       "retweet_count                       0\n",
       "text                                0\n",
       "tweet_coord                     13621\n",
       "tweet_created                       0\n",
       "tweet_location                   4733\n",
       "user_timezone                    4820\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificamos las columnas vacias\n",
    "data_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Eliminamos las columnas nulas\n",
    "del data_set[\"airline_sentiment_gold\"]\n",
    "del data_set[\"negativereason_gold\"]\n",
    "del data_set[\"negativereason_confidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@VirginAmerica This is such a great deal! Alre...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@VirginAmerica @virginmedia I'm flying your #f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text airline_sentiment\n",
       "1   @VirginAmerica plus you've added commercials t...          positive\n",
       "3   @VirginAmerica it's really aggressive to blast...          negative\n",
       "4   @VirginAmerica and it's a really big bad thing...          negative\n",
       "5   @VirginAmerica seriously would pay $30 a fligh...          negative\n",
       "6   @VirginAmerica yes, nearly every time I fly VX...          positive\n",
       "8     @virginamerica Well, I didn't…but NOW I DO! :-D          positive\n",
       "9   @VirginAmerica it was amazing, and arrived an ...          positive\n",
       "11  @VirginAmerica I &lt;3 pretty graphics. so muc...          positive\n",
       "12  @VirginAmerica This is such a great deal! Alre...          positive\n",
       "13  @VirginAmerica @virginmedia I'm flying your #f...          positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seleccionamos los tweets positivosy negativos para nuestra TFIDF usaremos solo 10 tweets\n",
    "tweets=data_set.loc[(data_set[\"airline_sentiment\"]==\"positive\") | (data_set[\"airline_sentiment\"]==\"negative\")]\n",
    "textdata=tweets[[\"text\",\"airline_sentiment\"]].head(10)\n",
    "textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Eliminamos caracteres inecesarios\n",
    "textdata=textdata['text'].str.replace(\"^@\\\\w+ *\", \"\", case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     plus you've added commercials to the experienc...\n",
       "3     it's really aggressive to blast obnoxious \"ent...\n",
       "4              and it's a really big bad thing about it\n",
       "5     seriously would pay $30 a flight for seats tha...\n",
       "6     yes, nearly every time I fly VX this “ear worm...\n",
       "8                      Well, I didn't…but NOW I DO! :-D\n",
       "9     it was amazing, and arrived an hour early. You...\n",
       "11    I &lt;3 pretty graphics. so much better than m...\n",
       "12    This is such a great deal! Already thinking ab...\n",
       "13    @virginmedia I'm flying your #fabulous #Seduct...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cada tweet o texto sera colocado en alldocuments \n",
    "alldocuments=[]\n",
    "for document in textdata.values:\n",
    "    alldocuments.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aplicamos una implementacion basica de tfidf para calcular que palabras son las mas importantes en los 10 tweets o documentos\n",
    "#y lo calculamos mediante la funcion sklearn_tfidf.fit_transform\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(alldocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36745042,  0.36745042,  0.24521722,  0.2132093 ,  0.36745042,\n",
       "        0.36745042,  0.36745042,  0.29032986,  0.36745042,  0.27743999,\n",
       "        0.27743999,  0.21921083,  0.27743999,  0.21921083,  0.27743999,\n",
       "        0.27743999,  0.21921083,  0.27743999,  0.27743999,  0.27743999,\n",
       "        0.27743999,  0.27743999,  0.18514896,  0.16098168,  0.16098168,\n",
       "        0.30688755,  0.36334569,  0.36334569,  0.45986151,  0.36334569,\n",
       "        0.30688755,  0.45178224,  0.24814937,  0.19606774,  0.24814937,\n",
       "        0.24814937,  0.16560193,  0.19606774,  0.24814937,  0.24814937,\n",
       "        0.24814937,  0.24814937,  0.24814937,  0.24814937,  0.24814937,\n",
       "        0.24814937,  0.16560193,  0.19606774,  0.19606774,  0.19606774,\n",
       "        0.16560193,  0.1439861 ,  0.16560193,  0.23747952,  0.3005614 ,\n",
       "        0.3005614 ,  0.3005614 ,  0.3005614 ,  0.3005614 ,  0.3005614 ,\n",
       "        0.3005614 ,  0.3005614 ,  0.3005614 ,  0.3005614 ,  0.20057898,\n",
       "        0.46502628,  0.46502628,  0.46502628,  0.46502628,  0.36742648,\n",
       "        0.28961872,  0.28961872,  0.28961872,  0.28961872,  0.28961872,\n",
       "        0.28961872,  0.28961872,  0.28961872,  0.28961872,  0.28961872,\n",
       "        0.22883349,  0.16804826,  0.16804826,  0.22883349,  0.33333333,\n",
       "        0.33333333,  0.33333333,  0.33333333,  0.33333333,  0.33333333,\n",
       "        0.33333333,  0.33333333,  0.33333333,  0.21523965,  0.21523965,\n",
       "        0.21523965,  0.21523965,  0.21523965,  0.21523965,  0.21523965,\n",
       "        0.36443241,  0.21523965,  0.36443241,  0.21523965,  0.21523965,\n",
       "        0.21523965,  0.21523965,  0.21523965,  0.21523965,  0.1436397 ,\n",
       "        0.1436397 ,  0.17006511,  0.12489057,  0.2555029 ,  0.2555029 ,\n",
       "        0.2555029 ,  0.2555029 ,  0.2555029 ,  0.2555029 ,  0.2555029 ,\n",
       "        0.2555029 ,  0.2555029 ,  0.2555029 ,  0.2555029 ,  0.2555029 ,\n",
       "        0.2555029 ,  0.2018779 ,  0.2018779 ,  0.2018779 ,  0.17050929])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostramos la informacion\n",
    "sklearn_representation.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Para la clasificacion bayesiana me tome la libertad de utilizar la libreria nltk, puesto que tengo mas dominio utilizando\n",
    "#esta libreria y la otra estoy en un proceso de aprendizaje aun\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.metrics\n",
    "#Las columnas seleccionadas son los tweets y el sentimiento que puede ser positivo o negativo\n",
    "tweetsclass=tweets[[\"text\",\"airline_sentiment\"]] \n",
    "tweetsclass=tweetsclass.reset_index(drop=True) #Reseteamos los indices\n",
    "#Separamos nuestros tweets de prueba y de entrenamiento\n",
    "tweets2=tweetsclass[6000:8000]\n",
    "tweets=tweetsclass.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Esta funcion me permite obtener todas las palabras de los tweets a analizar en una lista\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "#Esta funcion agrupa las palabras con distancias mas cortas por su frecuencia\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "\n",
    "def extract_features(names):\n",
    "    document_words = set(names)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clasificacion bayesiana\n",
    "tw=[]\n",
    "for i in range(len(tweets[\"text\"])):\n",
    "    #tokenizer\n",
    "    w=word_tokenize(tweets[\"text\"][i])\n",
    "    tw.append((w,tweets[\"airline_sentiment\"][i]))\n",
    "\n",
    "    \n",
    "word_features = get_word_features(get_words_in_tweets(tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#obtenemos nuestro conjunto de datos para entrenar el modelo\n",
    "training_set = nltk.classify.apply_features(extract_features, tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "#Aplicamos el metodo de clasificacion bayesiana\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "#Predecimos un tweet cualquiera\n",
    "tweet = 'I had a good flight'\n",
    "print (classifier.classify(extract_features(tweet.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
